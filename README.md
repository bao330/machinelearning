1. 模型介绍

1.1 梯度下降法

梯度下降就好比从一个凹凸不平的山顶快速下到山脚下，每一步都会根据当前的坡度来找一个能最快下来的方向,通过模拟小球滚动的方法来得到函数的最小值点。

1.2 线性回归器

线性回归器是最为简单、易用的回归模型。正是因为其对特征与回归目标之间的线性假设，从某种程度上说也局限了其应用范围。现实生活中的许多实例数据的各个特征与回归目标之间，绝大多数不能保证严格的线性关系。尽管如此，在不清楚特征之间关系的前提下，我们仍然可以使用线性回归模型作为大多数科学试验的基线系统。

1.3 支持向量机

支持向量机是一类按监督学习方式对数据进行二元分类的广义线性分类器，其决策边界是对学习样本求解的最大边距超平面，在人像识别、文本分类等模式识别问题中有所应用。

1.4 K近邻算法

K近邻分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：在特征空间中，如果一个样本附近的k个最近(即特征空间中最邻近)本的大多数属于某一个类别，则该样本也属于这个类别。

2. 项目展示

首先从sklearn.datasets导入波士顿房价数据，这个数据集包含了506处波士顿不同地理位置的房产的房价数据，和与之对应的包含房屋以及房屋周围的详细信息，其中包含城镇犯罪率、一氧化氮浓度、住宅平均房间数、到中心区域的加权距离以及自住房平均房价等13个维度的数据，本文用四种回归模型对数据进行训练学习以及预测。

2.1 数据处理

随机采样25%的数据构建测试样本，其余作为训练样本，并对训练数据与测试数据进行标准化处理。

2.2 线性回归

使用线性回归模型SGDRegressor（随机梯度下降线性回归）对美国波士顿地区房价进行预测，并从sklearn.metrics导入r2_score、mean_squared_error以及mean_absolute_error用于回归性能的评估。应用该回归模型对房价预测的结果与真实值的比较如下图所示：

<img src="1.png">

2.3 支持向量机回归

使用径向基核函数配置的支持向量机回归，并用R-squared、MSE和MAE指标进行性能评估。应用该回归模型对房价预测的结果与真实值的比较如下图所示：

<img src="2.png">

2.4 k近邻回归

使用据距离加权回归的K近邻回归模型对美国波士顿房价数据进行回归预测，并用R-squared、MSE以及MAE三种指标对根据距离加权回归配置的K近邻模型在测试集上进行性能评估。应用该回归模型对房价预测的结果与真实值的比较如下图所示：

<img src="3.png">

2.5 回归树回归

使用回归树对美国波士顿房价训练数据进行学习，并对测试数据进行预测，并用R-squared、MSE以及MAE指标对默认配置的回归树在测试集上进行性能评估。应用该模型对房价预测的结果与真实值的比较如下图所示：

<img src="4.png">

2.6 pca降维处理

用pca的方法对波士顿房价13个维度的数据进行降维处理，原始数据经降维处理后转换成3维的数据，再应用四种方法对波士顿地区房价进行预测。

3.结果

将四种模型对波士顿地区房价的预测精度导出，汇总如下：

<head>
	<meta charset="UTF-8">
</head>
<body>	
	<table border="1" ellspacing="0" cellpadding="5">
	<caption>降维前精度对比</caption>
	<tr>
		<td>方法</td>
		<td>R-squared</td>
		<td>MSE</td>
		<td>MAE</td>
    </tr>
    <tr>
		<td>线性回归</td>
		<td>0.66</td>
		<td>26.09</td>
		<td>3.52</td>
    </tr>
    <tr>
		<td>支持向量机回归</td>
		<td>0.76</td>
		<td>18.92</td>
		<td>2.61</td>
	</tr>
	<tr>
		<td>K近邻回归</td>
		<td>0.72</td>
		<td>21.70</td>
		<td>2.80</td>
    </tr>
    <tr>
		<td>回归树回归</td>
		<td>0.54</td>
		<td>35.70</td>
		<td>3.40</td>
    </tr>
</body>
</html></span>


<head>
	<meta charset="UTF-8">
</head>
<body>	
	<table border="1" ellspacing="0" cellpadding="5">
	<caption>降维后精度对比</caption>
	<tr>
		<td>方法</td>
		<td>R-squared</td>
		<td>MSE</td>
		<td>MAE</td>
    </tr>
    <tr>
		<td>线性回归</td>
		<td>0.38</td>
		<td>47.98</td>
		<td>4.98</td>
    </tr>
    <tr>
		<td>支持向量机回归</td>
		<td>0.43</td>
		<td>44.24</td>
		<td>4.42</td>
	</tr>
	<tr>
		<td>K近邻回归</td>
		<td>0.61</td>
		<td>30.34</td>
		<td>3.81</td>
    </tr>
    <tr>
		<td>回归树回归</td>
		<td>0.08</td>
		<td>71.00</td>
		<td>5.68</td>
    </tr>
</body>
</html></span>

<head>
	<meta charset="UTF-8">
</head>
<body>	
	<table border="1" ellspacing="0" cellpadding="5">
	<caption>降维后精度对比</caption>
	<tr>
		<td>方法</td>
		<td>R-squared</td>
		<td>MSE</td>
		<td>MAE</td>
    </tr>
    <tr>
		<td>线性回归</td>
		<td>0.38</td>
		<td>47.98</td>
		<td>4.98</td>
    </tr>
    <tr>
		<td>支持向量机回归</td>
		<td>0.43</td>
		<td>44.24</td>
		<td>4.42</td>
	</tr>
	<tr>
		<td>K近邻回归</td>
		<td>0.61</td>
		<td>30.34</td>
		<td>3.81</td>
    </tr>
    <tr>
		<td>回归树回归</td>
		<td>0.08</td>
		<td>71.00</td>
		<td>5.68</td>
    </tr>
</body>
</html></span>

在R²、MSE、MAE三种评价指标中，R²越小精度越高，而MSE、MAE则是数值越大，精度越高。由上述结果可知，横向比较来看，无论是降维前还是降维后，综合三种评价指标，回归树回归法的预测精度都要较其它三种方法高。而纵向比较来看，针对四种预测方法，无论是哪一项评价指标，降维后的预测精度都要较降维前的预测精度高。

综上得出结论：在美国波士顿房价预测案例中，数据经降维处理后效果更好。

4.感想

在这次的小组作业中，我们用线性回归器、支持向量机、K近邻等四种方法完成了美国波士顿地区房价数据预测的案例，通过对相关资料的查找以及对预测结果准确度的分析，我们发现，回归模型的三种评价方式，即R-squared、MSE以及MAE在具体评估结果上不同，但是在评价总体优劣程度的趋势上是一致的。

同时，虽然使用随机梯度下降估计参数的方法SGDRegressor在性能上不及使用解析方法的LinearRegression，但是如果面对训练数据规模十分庞大的任务，随机梯度法不论是在分类还是在回归问题上都表现得十分高效，可以再不损失过多性能的前提下，节省大量计算时间。

另外，线性回归是我们接触较早的回归类型，而在此次学习中，我们又进一步接触了最简单、易用的回归模型——线性回归器，它简单、易用的特点使得在平时学习中，当我们不清楚特征之间的关系时，就可以先使用线性回归模型作为试验的基线系统。但是线性回归器对特征与回归目标之间的线性假设，从某种程度上也局限了其应用范围，因为在现实生活中，许多实例数据的各个特征与回归目标之间，绝大多数不能保证严格的线性关系。

又比如，在学习过程中，案例就三种不同核函数配置下的支持向量机回归模型在相同测试集上的性能进行评估，发现存在着非常大的性能差异。为此，我们课后查找了核函数相关内容。核函数是一个二维向量的内积形式，可以等价于三维或者四维向量的内积，其中三维和四维向量是原来的二维向量通过一个映射函数获得的。它表示的是两个高维空间的向量内积可以通过计算低维空间的内积来获得，避免了“维数空难”，降低了计算量。

在线性可分的空间里，我们需要计算样本之间的内积， 所以核函数的作用总结如下：当我们在解决线性不可分的问题时，我们需要通过一个映射函数，把样本值映射到更高维的空间或者无穷维。在特征空间中，我们对线性可分的新样本使用前面提到过的求解线性可分的情况下的分类问题的方法时，需要计算样本内积，但是因为样本维数很高，容易造成“维数灾难”，所以这里我们就引入了核函数，把高维向量的内积转变成了求低维向量的内积问题。

可见，任何方法、模型，其优势与劣势都是共存的，即使是对同一个模型，其内部如果采用不同的函数配置，最终运行结果与精确度也是大相径庭，因此，在机器学习以及其它内容的学习过程中，我们应善于利用多种方式方法处理问题，以寻求突破，或许会收获不一样的效果！

